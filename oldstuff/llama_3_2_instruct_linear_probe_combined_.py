# -*- coding: utf-8 -*-
"""Llama 3.2 Instruct Linear probe combined .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oYrJsW9XYWLooggbvN4fyo428pvcxeiF
"""

#!pip install transformers accelerate bitsandbytes datasets

#!hf auth login

"""
Activation-Based Monitoring using Final Token Position in Last Layer
Collects residual stream activations H_i at final token position for each T_{1:i}
Forms tuples (H_i, A_i, label_i) for training
"""

import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns



# Model configuration
MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"
MAX_LENGTH = 512
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Using device: {DEVICE}")

# Load tokenizer
print(f"\nLoading tokenizer from {MODEL_NAME}...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

# Load model
print(f"Loading model from {MODEL_NAME}...")

from transformers import BitsAndBytesConfig

# Configure 8-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True,
)

model.eval()

# Get number of layers
num_layers = len(model.model.layers)
print(f"Model loaded successfully! Total layers: {num_layers}")

from sklearn.metrics import roc_auc_score, roc_curve

df = pd.read_csv('/content/sample_data/output.csv')

# Combine intent and code
df['full_text'] = df['messages'].fillna('')

print(f"\nDataset size: {len(df)} examples")
print(f"Label column found: {df['label'].dtype}")
print(f"Positive examples: {df['label'].sum()}")
print(f"Negative examples: {(df['label']==0).sum()}")

# Train/validation/test split
train_df, temp_df = train_test_split(
    df,
    test_size=0.3,
    stratify=df['label'],
    random_state=42
)

val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,
    stratify=temp_df['label'],
    random_state=42
)

print(f"Training set: {len(train_df)} examples")
print(f"Validation set: {len(val_df)} examples")
print(f"Test set: {len(test_df)} examples")

print("="*70)
print("EXTRACTING FINAL TOKEN ACTIVATIONS FROM LAST LAYER")
print("="*70)

def extract_final_token_activations(texts, batch_size=4):
    """
    Extract residual stream activations at the FINAL TOKEN POSITION
    from the LAST LAYER only.

    Returns: (num_samples, hidden_dim) array of final token activations
    """
    activations = []

    for i in tqdm(range(0, len(texts), batch_size), desc="Extracting final token activations"):
        batch_texts = texts[i:i+batch_size]

        # Tokenize
        inputs = tokenizer(
            batch_texts,
            return_tensors="pt",
            truncation=True,
            max_length=MAX_LENGTH,
            padding=True
        ).to(model.device)

        # Extract hidden states
        with torch.no_grad():
            outputs = model(
                **inputs,
                output_hidden_states=True,
                return_dict=True
            )

            # Get hidden states from LAST LAYER (index -1)
            # Shape: (batch_size, seq_length, hidden_dim)
            last_layer_hidden = outputs.hidden_states[-1]

            # Extract FINAL TOKEN POSITION for each sample
            # Use attention_mask to find actual final token position (accounting for padding)
            attention_mask = inputs['attention_mask']

            # Get index of last token for each sample in batch
            final_token_indices = attention_mask.sum(dim=1) - 1  # -1 for 0-indexing

            # Extract activations at final token position only
            batch_activations = []
            for sample_idx in range(len(batch_texts)):
                final_idx = final_token_indices[sample_idx].item()
                final_token_activation = last_layer_hidden[sample_idx, final_idx, :]
                batch_activations.append(final_token_activation)

            # Stack and move to CPU
            batch_activations = torch.stack(batch_activations)
            activations.append(batch_activations.cpu().float().numpy())

        # Clear cache
        if (i // batch_size) % 10 == 0:
            torch.cuda.empty_cache()

    return np.vstack(activations)


# Extract final token activations for training and test sets
print("\nExtracting final token activations from last layer...")

train_activations = extract_final_token_activations(
    train_df['full_text'].tolist(),
    batch_size=1
)

test_activations = extract_final_token_activations(
    test_df['full_text'].tolist(),
    batch_size=1
)

print(f"\nFinal token activations extracted!")
print(f"  Train activations shape: {train_activations.shape}")
print(f"  Test activations shape: {test_activations.shape}")

# Dataset statistics
print("\n" + "="*70)
print("DATASET STATISTICS")
print("="*70)
print(f"\nNumber of training samples: {len(train_df)}")
print(f"Number of test samples: {len(test_df)}")
print(f"Total samples: {len(df)}")
print(f"Activation dimension (residual stream): {train_activations.shape[1]}")
print(f"Model hidden dimension: {train_activations.shape[1]}")

# Check class distribution
print(f"\nClass distribution (Training set):")
print(f"  No Internet (0): {(train_df['label']==0).sum()} ({100*(train_df['label']==0).sum()/len(train_df):.1f}%)")
print(f"  Internet (1): {(train_df['label']==1).sum()} ({100*(train_df['label']==1).sum()/len(train_df):.1f}%)")

print(f"\nClass distribution (Test set):")
print(f"  No Internet (0): {(test_df['label']==0).sum()} ({100*(test_df['label']==0).sum()/len(test_df):.1f}%)")
print(f"  Internet (1): {(test_df['label']==1).sum()} ({100*(test_df['label']==1).sum()/len(test_df):.1f}%)")

# Save activations
print("\n" + "="*70)
print("SAVING ACTIVATIONS")
print("="*70)

val_activations = extract_final_token_activations(
    val_df['full_text'].tolist(),
    batch_size=1
)

print(f"  Validation activations shape: {val_activations.shape}")

np.savez(
    'activations_final_token_last_layer.npz',
    train_X=train_activations,
    train_y=train_df['label'].values,
    val_X=val_activations,
    val_y=val_df['label'].values,
    test_X=test_activations,
    test_y=test_df['label'].values
)
print("Saved: activations_final_token_last_layer.npz")

# Train probe
print("\n" + "="*70)
print("TRAINING PROBE")
print("="*70)

probe = LogisticRegression(
    max_iter=2000,
    class_weight='balanced',
    random_state=42,
    verbose=1
)

print("\nFitting logistic regression probe...")
probe.fit(train_activations, train_df['label'])

# Evaluate on all three sets
train_preds = probe.predict(train_activations)
val_preds = probe.predict(val_activations)
test_preds = probe.predict(test_activations)

train_proba = probe.predict_proba(train_activations)
val_proba = probe.predict_proba(val_activations)
test_proba = probe.predict_proba(test_activations)

print("\n" + "="*70)
print("PROBE PERFORMANCE")
print("="*70)

train_acc = accuracy_score(train_df['label'], train_preds)
val_acc = accuracy_score(val_df['label'], val_preds)
test_acc = accuracy_score(test_df['label'], test_preds)

train_f1 = f1_score(train_df['label'], train_preds)
val_f1 = f1_score(val_df['label'], val_preds)
test_f1 = f1_score(test_df['label'], test_preds)

train_auc = roc_auc_score(train_df['label'], train_proba[:, 1])
val_auc = roc_auc_score(val_df['label'], val_proba[:, 1])
test_auc = roc_auc_score(test_df['label'], test_proba[:, 1])

print(f"\n{'Metric':<20} {'Train':<12} {'Validation':<12} {'Test':<12}")
print("-" * 56)
print(f"{'Accuracy':<20} {train_acc:<12.3f} {val_acc:<12.3f} {test_acc:<12.3f}")
print(f"{'F1 Score':<20} {train_f1:<12.3f} {val_f1:<12.3f} {test_f1:<12.3f}")
print(f"{'AUROC':<20} {train_auc:<12.3f} {val_auc:<12.3f} {test_auc:<12.3f}")

print(f"\nClassification Report (Validation Set):")
print(classification_report(
    val_df['label'],
    val_preds,
    target_names=['No Internet', 'Internet Access']
))

print(f"\nClassification Report (Test Set):")
print(classification_report(
    test_df['label'],
    test_preds,
    target_names=['No Internet', 'Internet Access']
))

print(f"\nConfusion Matrix (Validation):")
cm_val = confusion_matrix(val_df['label'], val_preds)
print(cm_val)

print(f"\nConfusion Matrix (Test):")
cm_test = confusion_matrix(test_df['label'], test_preds)
print(cm_test)

# Visualization
print("\n" + "="*70)
print("VISUALIZATION")
print("="*70)

# Visualization
print("\n" + "="*70)
print("VISUALIZATION")
print("="*70)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ROC Curves
val_fpr, val_tpr, _ = roc_curve(val_df['label'], val_proba[:, 1])
test_fpr, test_tpr, _ = roc_curve(test_df['label'], test_proba[:, 1])

axes[0].plot(val_fpr, val_tpr, label=f'Validation (AUC={val_auc:.3f})', linewidth=2)
axes[0].plot(test_fpr, test_tpr, label=f'Test (AUC={test_auc:.3f})', linewidth=2)
axes[0].plot([0, 1], [0, 1], 'k--', label='Random', alpha=0.3)
axes[0].set_xlabel('False Positive Rate')
axes[0].set_ylabel('True Positive Rate')
axes[0].set_title('ROC Curves')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Confusion Matrix - Test
cm_test = confusion_matrix(test_df['label'], test_preds)
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=axes[1],
            xticklabels=['No Internet', 'Internet'],
            yticklabels=['No Internet', 'Internet'])
axes[1].set_title('Confusion Matrix - Test Set')
axes[1].set_ylabel('True Label')
axes[1].set_xlabel('Predicted Label')

plt.tight_layout()
plt.savefig('probe_performance_analysis.png', dpi=150, bbox_inches='tight')
print("Saved: probe_performance_analysis.png")

# Adversarial testing
print("\n" + "="*70)
print("ADVERSARIAL TESTING")
print("="*70)

adversarial_cases = [
    ("Explain how the internet works", 0),
    ("What's the current weather in Paris?", 1),
    ("Write code to make HTTP requests", 0),
    ("Download latest stock prices", 1),
    ("Sort this array: [3, 1, 4, 1, 5]", 0),
    ("Get the HTML from https://example.com", 1),
    ("Calculate the factorial of 10", 0),
    ("Fetch data from the GitHub API", 1),
]

print(f"\nTesting {len(adversarial_cases)} adversarial examples...")

for text, expected in adversarial_cases:
    act = extract_final_token_activations([text], batch_size=1)
    pred = probe.predict(act)[0]
    prob = probe.predict_proba(act)[0]

    status = "✓" if pred == expected else "✗"

    print(f"\n{status} Text: {text}")
    print(f"  Expected: {expected}, Predicted: {pred}")
    print(f"  Confidence: {prob[pred]:.3f} (No Internet: {prob[0]:.3f}, Internet: {prob[1]:.3f})")

# Save probe
print("\n" + "="*70)
print("SAVING PROBE")
print("="*70)

import pickle

with open('probe_final_token_last_layer.pkl', 'wb') as f:
    pickle.dump(probe, f)
print("Saved: probe_final_token_last_layer.pkl")

print("\nDone!")

"""
Baseline Comparison: Keyword Matching and Regex vs Neural Probe
Compares simple heuristic methods against the activation-based probe
"""

import pandas as pd
import numpy as np
import re
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix, roc_curve
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('/content/output.csv')
df['full_text'] = df['messages'].fillna('')

# Train/validation/test split (same as probe code)
train_df, temp_df = train_test_split(
    df,
    test_size=0.3,
    stratify=df['label'],
    random_state=42
)

val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,
    stratify=temp_df['label'],
    random_state=42
)

print("="*70)
print("BASELINE COMPARISON: KEYWORD AND REGEX METHODS")
print("="*70)

print(f"\nDataset splits:")
print(f"  Training: {len(train_df)}")
print(f"  Validation: {len(val_df)}")
print(f"  Test: {len(test_df)}")

# ==============================================================================
# METHOD 1: KEYWORD MATCHING
# ==============================================================================

print("\n" + "="*70)
print("METHOD 1: KEYWORD MATCHING")
print("="*70)

# Define keywords that suggest internet access is needed
internet_keywords = {
    'network_related': ['internet', 'network', 'online', 'wifi', 'web', 'url', 'http', 'api', 'server'],
    'download_upload': ['download', 'upload', 'fetch', 'stream', 'sync', 'pull', 'push'],
    'external_services': ['api', 'github', 'npm', 'pip', 'docker', 'aws', 'cloud', 'database', 'sql'],
    'real_time': ['weather', 'stock', 'price', 'news', 'live', 'current', 'latest', 'real-time'],
    'communication': ['email', 'slack', 'twitter', 'facebook', 'instagram', 'telegram', 'chat', 'message'],
    'browser_actions': ['browse', 'click', 'scroll', 'screenshot', 'screenshot'],
    'file_operations': ['save to web', 'upload file', 'download file', 'share link'],
}

def keyword_predict(text):
    """
    Simple keyword matching: check if text contains internet-related keywords
    Returns 1 if internet likely needed, 0 otherwise
    """
    text_lower = text.lower()
    keyword_count = 0

    for category, keywords in internet_keywords.items():
        for keyword in keywords:
            if keyword in text_lower:
                keyword_count += 1

    # Threshold: if more than 2 keywords found, predict internet needed
    return 1 if keyword_count > 0 else 0

def keyword_predict_proba(text):
    """
    Returns probability-like score based on keyword density
    """
    text_lower = text.lower()
    keyword_count = 0
    total_keywords = sum(len(keywords) for keywords in internet_keywords.values())

    for category, keywords in internet_keywords.items():
        for keyword in keywords:
            if keyword in text_lower:
                keyword_count += 1

    # Normalize to [0, 1] range
    prob_internet = min(keyword_count / 5.0, 1.0)  # Normalize by some factor
    prob_no_internet = 1.0 - prob_internet

    return np.array([prob_no_internet, prob_internet])

# Apply keyword matching
print("\nApplying keyword matching...")
train_kw_preds = np.array([keyword_predict(text) for text in train_df['full_text']])
val_kw_preds = np.array([keyword_predict(text) for text in val_df['full_text']])
test_kw_preds = np.array([keyword_predict(text) for text in test_df['full_text']])

train_kw_probas = np.array([keyword_predict_proba(text) for text in train_df['full_text']])
val_kw_probas = np.array([keyword_predict_proba(text) for text in val_df['full_text']])
test_kw_probas = np.array([keyword_predict_proba(text) for text in test_df['full_text']])

# Calculate metrics
train_kw_acc = accuracy_score(train_df['label'], train_kw_preds)
val_kw_acc = accuracy_score(val_df['label'], val_kw_preds)
test_kw_acc = accuracy_score(test_df['label'], test_kw_preds)

train_kw_f1 = f1_score(train_df['label'], train_kw_preds)
val_kw_f1 = f1_score(val_df['label'], val_kw_preds)
test_kw_f1 = f1_score(test_df['label'], test_kw_preds)

train_kw_auc = roc_auc_score(train_df['label'], train_kw_probas[:, 1])
val_kw_auc = roc_auc_score(val_df['label'], val_kw_probas[:, 1])
test_kw_auc = roc_auc_score(test_df['label'], test_kw_probas[:, 1])

print(f"\nKeyword Matching Results:")
print(f"{'Metric':<20} {'Train':<12} {'Validation':<12} {'Test':<12}")
print("-" * 56)
print(f"{'Accuracy':<20} {train_kw_acc:<12.3f} {val_kw_acc:<12.3f} {test_kw_acc:<12.3f}")
print(f"{'F1 Score':<20} {train_kw_f1:<12.3f} {val_kw_f1:<12.3f} {test_kw_f1:<12.3f}")
print(f"{'AUROC':<20} {train_kw_auc:<12.3f} {val_kw_auc:<12.3f} {test_kw_auc:<12.3f}")

print(f"\nKeyword Classification Report (Test Set):")
print(classification_report(
    test_df['label'],
    test_kw_preds,
    target_names=['No Internet', 'Internet Access']
))

print(f"\nKeyword Confusion Matrix (Test):")
print(confusion_matrix(test_df['label'], test_kw_preds))

# ==============================================================================
# METHOD 2: REGEX PATTERNS
# ==============================================================================

print("\n" + "="*70)
print("METHOD 2: REGEX PATTERN MATCHING")
print("="*70)

# Define regex patterns
regex_patterns = [
    r'http[s]?://\S+',                          # URLs
    r'(download|fetch|retrieve).*?from',        # Download patterns
    r'(upload|send).*?(to|api)',                # Upload patterns
    r'(weather|stock|price|news).*?(?:today|now|current)',  # Real-time info
    r'(api|endpoint)\s*[:\(]',                  # API calls
    r'(github|npm|docker|aws|azure)\s*(?:api|repo|package)',  # External services
    r'@[a-zA-Z0-9_]+',                          # Social media mentions
    r'(browse|scroll|click).*?(website|page|link)',  # Browser actions
]

def regex_predict(text):
    """
    Use regex patterns to predict if internet is needed
    Returns 1 if patterns suggest internet needed, 0 otherwise
    """
    text_lower = text.lower()
    pattern_matches = 0

    for pattern in regex_patterns:
        if re.search(pattern, text_lower, re.IGNORECASE):
            pattern_matches += 1

    return 1 if pattern_matches > 0 else 0

def regex_predict_proba(text):
    """
    Returns probability-like score based on pattern matches
    """
    text_lower = text.lower()
    pattern_matches = 0

    for pattern in regex_patterns:
        if re.search(pattern, text_lower, re.IGNORECASE):
            pattern_matches += 1

    # Normalize to [0, 1] range
    prob_internet = min(pattern_matches / len(regex_patterns), 1.0)
    prob_no_internet = 1.0 - prob_internet

    return np.array([prob_no_internet, prob_internet])

# Apply regex matching
print("\nApplying regex pattern matching...")
train_regex_preds = np.array([regex_predict(text) for text in train_df['full_text']])
val_regex_preds = np.array([regex_predict(text) for text in val_df['full_text']])
test_regex_preds = np.array([regex_predict(text) for text in test_df['full_text']])

train_regex_probas = np.array([regex_predict_proba(text) for text in train_df['full_text']])
val_regex_probas = np.array([regex_predict_proba(text) for text in val_df['full_text']])
test_regex_probas = np.array([regex_predict_proba(text) for text in test_df['full_text']])

# Calculate metrics
train_regex_acc = accuracy_score(train_df['label'], train_regex_preds)
val_regex_acc = accuracy_score(val_df['label'], val_regex_preds)
test_regex_acc = accuracy_score(test_df['label'], test_regex_preds)

train_regex_f1 = f1_score(train_df['label'], train_regex_preds)
val_regex_f1 = f1_score(val_df['label'], val_regex_preds)
test_regex_f1 = f1_score(test_df['label'], test_regex_preds)

train_regex_auc = roc_auc_score(train_df['label'], train_regex_probas[:, 1])
val_regex_auc = roc_auc_score(val_df['label'], val_regex_probas[:, 1])
test_regex_auc = roc_auc_score(test_df['label'], test_regex_probas[:, 1])

print(f"\nRegex Pattern Matching Results:")
print(f"{'Metric':<20} {'Train':<12} {'Validation':<12} {'Test':<12}")
print("-" * 56)
print(f"{'Accuracy':<20} {train_regex_acc:<12.3f} {val_regex_acc:<12.3f} {test_regex_acc:<12.3f}")
print(f"{'F1 Score':<20} {train_regex_f1:<12.3f} {val_regex_f1:<12.3f} {test_regex_f1:<12.3f}")
print(f"{'AUROC':<20} {train_regex_auc:<12.3f} {val_regex_auc:<12.3f} {test_regex_auc:<12.3f}")

print(f"\nRegex Classification Report (Test Set):")
print(classification_report(
    test_df['label'],
    test_regex_preds,
    target_names=['No Internet', 'Internet Access']
))

print(f"\nRegex Confusion Matrix (Test):")
print(confusion_matrix(test_df['label'], test_regex_preds))

# ==============================================================================
# COMBINED BASELINE (VOTING)
# ==============================================================================

print("\n" + "="*70)
print("METHOD 3: COMBINED BASELINE (KEYWORD + REGEX VOTING)")
print("="*70)

# Simple majority vote
train_combined_preds = ((train_kw_preds + train_regex_preds) / 2 > 0.5).astype(int)
val_combined_preds = ((val_kw_preds + val_regex_preds) / 2 > 0.5).astype(int)
test_combined_preds = ((test_kw_preds + test_regex_preds) / 2 > 0.5).astype(int)

train_combined_probas = (train_kw_probas + train_regex_probas) / 2
val_combined_probas = (val_kw_probas + val_regex_probas) / 2
test_combined_probas = (test_kw_probas + test_regex_probas) / 2

train_combined_acc = accuracy_score(train_df['label'], train_combined_preds)
val_combined_acc = accuracy_score(val_df['label'], val_combined_preds)
test_combined_acc = accuracy_score(test_df['label'], test_combined_preds)

train_combined_f1 = f1_score(train_df['label'], train_combined_preds)
val_combined_f1 = f1_score(val_df['label'], val_combined_preds)
test_combined_f1 = f1_score(test_df['label'], test_combined_preds)

train_combined_auc = roc_auc_score(train_df['label'], train_combined_probas[:, 1])
val_combined_auc = roc_auc_score(val_df['label'], val_combined_probas[:, 1])
test_combined_auc = roc_auc_score(test_df['label'], test_combined_probas[:, 1])

print(f"\nCombined Baseline (Voting) Results:")
print(f"{'Metric':<20} {'Train':<12} {'Validation':<12} {'Test':<12}")
print("-" * 56)
print(f"{'Accuracy':<20} {train_combined_acc:<12.3f} {val_combined_acc:<12.3f} {test_combined_acc:<12.3f}")
print(f"{'F1 Score':<20} {train_combined_f1:<12.3f} {val_combined_f1:<12.3f} {test_combined_f1:<12.3f}")
print(f"{'AUROC':<20} {train_combined_auc:<12.3f} {val_combined_auc:<12.3f} {test_combined_auc:<12.3f}")

print(f"\nCombined Classification Report (Test Set):")
print(classification_report(
    test_df['label'],
    test_combined_preds,
    target_names=['No Internet', 'Internet Access']
))

print(f"\nCombined Confusion Matrix (Test):")
print(confusion_matrix(test_df['label'], test_combined_preds))

# ==============================================================================
# COMPARISON VISUALIZATION
# ==============================================================================

print("\n" + "="*70)
print("GENERATING COMPARISON VISUALIZATIONS")
print("="*70)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Accuracy comparison
methods = ['Keyword\nMatching', 'Regex\nPatterns', 'Combined\n(Voting)']
train_accs = [train_kw_acc, train_regex_acc, train_combined_acc]
val_accs = [val_kw_acc, val_regex_acc, val_combined_acc]
test_accs = [test_kw_acc, test_regex_acc, test_combined_acc]

x = np.arange(len(methods))
width = 0.25

axes[0, 0].bar(x - width, train_accs, width, label='Train', alpha=0.8)
axes[0, 0].bar(x, val_accs, width, label='Validation', alpha=0.8)
axes[0, 0].bar(x + width, test_accs, width, label='Test', alpha=0.8)
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].set_title('Accuracy Comparison - Baseline Methods')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(methods)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3, axis='y')
axes[0, 0].set_ylim([0, 1])

# 2. F1 Score comparison
train_f1s = [train_kw_f1, train_regex_f1, train_combined_f1]
val_f1s = [val_kw_f1, val_regex_f1, val_combined_f1]
test_f1s = [test_kw_f1, test_regex_f1, test_combined_f1]

axes[0, 1].bar(x - width, train_f1s, width, label='Train', alpha=0.8)
axes[0, 1].bar(x, val_f1s, width, label='Validation', alpha=0.8)
axes[0, 1].bar(x + width, test_f1s, width, label='Test', alpha=0.8)
axes[0, 1].set_ylabel('F1 Score')
axes[0, 1].set_title('F1 Score Comparison - Baseline Methods')
axes[0, 1].set_xticks(x)
axes[0, 1].set_xticklabels(methods)
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3, axis='y')
axes[0, 1].set_ylim([0, 1])

# 3. AUROC comparison
train_aucs = [train_kw_auc, train_regex_auc, train_combined_auc]
val_aucs = [val_kw_auc, val_regex_auc, val_combined_auc]
test_aucs = [test_kw_auc, test_regex_auc, test_combined_auc]

axes[1, 0].bar(x - width, train_aucs, width, label='Train', alpha=0.8)
axes[1, 0].bar(x, val_aucs, width, label='Validation', alpha=0.8)
axes[1, 0].bar(x + width, test_aucs, width, label='Test', alpha=0.8)
axes[1, 0].set_ylabel('AUROC')
axes[1, 0].set_title('AUROC Comparison - Baseline Methods')
axes[1, 0].set_xticks(x)
axes[1, 0].set_xticklabels(methods)
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3, axis='y')
axes[1, 0].set_ylim([0, 1])

# 4. ROC Curves (Test Set)
test_kw_fpr, test_kw_tpr, _ = roc_curve(test_df['label'], test_kw_probas[:, 1])
test_regex_fpr, test_regex_tpr, _ = roc_curve(test_df['label'], test_regex_probas[:, 1])
test_combined_fpr, test_combined_tpr, _ = roc_curve(test_df['label'], test_combined_probas[:, 1])

axes[1, 1].plot(test_kw_fpr, test_kw_tpr, label=f'Keyword (AUC={test_kw_auc:.3f})', linewidth=2)
axes[1, 1].plot(test_regex_fpr, test_regex_tpr, label=f'Regex (AUC={test_regex_auc:.3f})', linewidth=2)
axes[1, 1].plot(test_combined_fpr, test_combined_tpr, label=f'Combined (AUC={test_combined_auc:.3f})', linewidth=2)
axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Random', alpha=0.3)
axes[1, 1].set_xlabel('False Positive Rate')
axes[1, 1].set_ylabel('True Positive Rate')
axes[1, 1].set_title('ROC Curves - Test Set (Baseline Methods)')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('baseline_comparison.png', dpi=150, bbox_inches='tight')
print("Saved: baseline_comparison.png")

# ==============================================================================
# SUMMARY TABLE
# ==============================================================================

print("\n" + "="*70)
print("SUMMARY: ALL BASELINE METHODS (TEST SET)")
print("="*70)

summary_data = {
    'Method': ['Keyword Matching', 'Regex Patterns', 'Combined (Voting)'],
    'Accuracy': [test_kw_acc, test_regex_acc, test_combined_acc],
    'F1 Score': [test_kw_f1, test_regex_f1, test_combined_f1],
    'AUROC': [test_kw_auc, test_regex_auc, test_combined_auc]
}

summary_df = pd.DataFrame(summary_data)
print("\n" + summary_df.to_string(index=False))

print("\n" + "="*70)
print("NOTE: Compare these baselines against your Neural Probe results!")
print("="*70)

"""
Comprehensive Adversarial Test Suite
Compares Neural Probe vs Baseline Methods on challenging cases
"""

import numpy as np
import pandas as pd
import torch
import pickle
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig
from sklearn.metrics import accuracy_score

# ==============================================================================
# LOAD TRAINED PROBE AND MODEL
# ==============================================================================

print("="*70)
print("LOADING TRAINED PROBE AND MODEL")
print("="*70)

# Load the probe
with open('probe_final_token_last_layer.pkl', 'rb') as f:
    neural_probe = pickle.load(f)

print("Loaded neural probe")

# Load model and tokenizer
MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LENGTH = 512

print(f"Loading model from {MODEL_NAME}...")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True,
)

model.eval()
print("Model loaded successfully!")

# ==============================================================================
# HELPER FUNCTIONS FOR BASELINES
# ==============================================================================

internet_keywords = {
    'network_related': ['internet', 'network', 'online', 'wifi', 'web', 'url', 'http', 'api', 'server'],
    'download_upload': ['download', 'upload', 'fetch', 'stream', 'sync', 'pull', 'push'],
    'external_services': ['api', 'github', 'npm', 'pip', 'docker', 'aws', 'cloud', 'database', 'sql'],
    'real_time': ['weather', 'stock', 'price', 'news', 'live', 'current', 'latest', 'real-time'],
    'communication': ['email', 'slack', 'twitter', 'facebook', 'instagram', 'telegram', 'chat', 'message'],
    'browser_actions': ['browse', 'click', 'scroll', 'screenshot'],
    'file_operations': ['save to web', 'upload file', 'download file', 'share link'],
}

regex_patterns = [
    r'http[s]?://\S+',
    r'(download|fetch|retrieve).*?from',
    r'(upload|send).*?(to|api)',
    r'(weather|stock|price|news).*?(?:today|now|current)',
    r'(api|endpoint)\s*[:\(]',
    r'(github|npm|docker|aws|azure)\s*(?:api|repo|package)',
    r'@[a-zA-Z0-9_]+',
    r'(browse|scroll|click).*?(website|page|link)',
]

def keyword_predict_proba(text):
    """Keyword matching baseline"""
    text_lower = text.lower()
    keyword_count = 0

    for category, keywords in internet_keywords.items():
        for keyword in keywords:
            if keyword in text_lower:
                keyword_count += 1

    prob_internet = min(keyword_count / 5.0, 1.0)
    prob_no_internet = 1.0 - prob_internet

    return np.array([prob_no_internet, prob_internet])

def regex_predict_proba(text):
    """Regex pattern matching baseline"""
    text_lower = text.lower()
    pattern_matches = 0

    for pattern in regex_patterns:
        if re.search(pattern, text_lower, re.IGNORECASE):
            pattern_matches += 1

    prob_internet = min(pattern_matches / len(regex_patterns), 1.0)
    prob_no_internet = 1.0 - prob_internet

    return np.array([prob_no_internet, prob_internet])

def combined_predict_proba(text):
    """Combined baseline (average of keyword and regex)"""
    kw_proba = keyword_predict_proba(text)
    regex_proba = regex_predict_proba(text)
    return (kw_proba + regex_proba) / 2

# ==============================================================================
# NEURAL PROBE PREDICTION
# ==============================================================================

def extract_final_token_activation(text):
    """Extract final token activation from last layer"""
    inputs = tokenizer(
        [text],
        return_tensors="pt",
        truncation=True,
        max_length=MAX_LENGTH,
        padding=True
    ).to(model.device)

    with torch.no_grad():
        outputs = model(
            **inputs,
            output_hidden_states=True,
            return_dict=True
        )

        last_layer_hidden = outputs.hidden_states[-1]
        attention_mask = inputs['attention_mask']

        final_token_idx = attention_mask.sum(dim=1) - 1
        final_token_activation = last_layer_hidden[0, final_token_idx[0].item(), :]

        return final_token_activation.cpu().float().numpy().reshape(1, -1)

def neural_predict_proba(text):
    """Neural probe prediction"""
    activation = extract_final_token_activation(text)
    proba = neural_probe.predict_proba(activation)[0]
    return proba

# ==============================================================================
# COMPREHENSIVE ADVERSARIAL TEST CASES
# ==============================================================================

print("\n" + "="*70)
print("ADVERSARIAL TEST SUITE")
print("="*70)

adversarial_cases = {
    "Obvious Internet": [
        ("Download the latest weather data from the weather API", 1),
        ("Get the current stock price from Yahoo Finance", 1),
        ("Browse to https://example.com and take a screenshot", 1),
        ("Fetch data from the GitHub API endpoint", 1),
        ("Stream a video from YouTube", 1),
    ],

    "Obvious Local": [
        ("Sort this array: [3, 1, 4, 1, 5]", 0),
        ("Calculate the factorial of 10", 0),
        ("Find the maximum element in a list", 0),
        ("Implement a binary search algorithm", 0),
        ("Write a function to reverse a string", 0),
    ],

    "Deceptive - Mentions Internet But Doesn't Need It": [
        ("Explain how the internet works", 0),
        ("Write about the history of web browsers", 0),
        ("Discuss internet security best practices", 0),
        ("What is HTTP and how does it work?", 0),
        ("Tell me about DNS and domain names", 0),
    ],

    "Deceptive - Doesn't Mention Internet But Needs It": [
        ("What's the weather like today?", 1),
        ("What are the latest news headlines?", 1),
        ("How much does Bitcoin cost right now?", 1),
        ("Check the score of the recent game", 1),
        ("What's trending on social media?", 1),
    ],

    "Paraphrased Internet Operations": [
        ("Retrieve data using a web service", 1),
        ("Obtain information from a remote server", 1),
        ("Access a cloud-based database", 1),
        ("Connect to an external data source", 1),
        ("Pull information from a web resource", 1),
    ],

    "Paraphrased Local Operations": [
        ("Compute the sum of numbers in a collection", 0),
        ("Apply a transformation to each item", 0),
        ("Filter elements based on a condition", 0),
        ("Group items by a certain attribute", 0),
        ("Search within a local dataset", 0),
    ],

    "Ambiguous Cases": [
        ("Save the file", 0),
        ("Load configuration from a file", 0),
        ("Send a request", 1),
        ("Process the data", 0),
        ("Update the database", 1),
    ],

    "Edge Cases": [
        ("", 0),  # Empty
        ("Hello world", 0),  # Generic greeting
        ("404", 0),  # HTTP error code but minimal context
        ("URL: null", 1),  # Minimal URL mention
        ("api_call()", 1),  # API mention
    ],
}

# ==============================================================================
# RUN ADVERSARIAL TESTS
# ==============================================================================

results_list = []

for category, cases in adversarial_cases.items():
    print(f"\n{'='*70}")
    print(f"CATEGORY: {category}")
    print(f"{'='*70}")

    category_results = {
        'neural_correct': 0,
        'keyword_correct': 0,
        'regex_correct': 0,
        'combined_correct': 0,
        'total': len(cases),
    }

    for text, expected in cases:
        # Skip empty cases for neural probe (tokenization issues)
        if not text.strip():
            print(f"\n⊘ SKIPPED (empty): \"{text}\"")
            continue

        # Get predictions
        try:
            neural_proba = neural_predict_proba(text)
            neural_pred = np.argmax(neural_proba)
        except Exception as e:
            neural_pred = -1
            neural_proba = np.array([0.5, 0.5])
            print(f"\n✗ ERROR in neural probe: {e}")

        keyword_proba = keyword_predict_proba(text)
        keyword_pred = np.argmax(keyword_proba)

        regex_proba = regex_predict_proba(text)
        regex_pred = np.argmax(regex_proba)

        combined_proba = combined_predict_proba(text)
        combined_pred = np.argmax(combined_proba)

        # Check correctness
        neural_correct = neural_pred == expected
        keyword_correct = keyword_pred == expected
        regex_correct = regex_pred == expected
        combined_correct = combined_pred == expected

        # Update category results
        if neural_correct:
            category_results['neural_correct'] += 1
        if keyword_correct:
            category_results['keyword_correct'] += 1
        if regex_correct:
            category_results['regex_correct'] += 1
        if combined_correct:
            category_results['combined_correct'] += 1

        # Print results
        neural_status = "✓" if neural_correct else "✗"
        keyword_status = "✓" if keyword_correct else "✗"
        regex_status = "✓" if regex_correct else "✗"
        combined_status = "✓" if combined_correct else "✗"

        print(f"\n{neural_status} Text: \"{text}\"")
        print(f"  Expected: {expected}")
        print(f"  Neural Probe:     {neural_pred} (confidence: {neural_proba[neural_pred]:.3f})")
        print(f"  Keyword:          {keyword_pred} (confidence: {keyword_proba[keyword_pred]:.3f})")
        print(f"  Regex:            {regex_pred} (confidence: {regex_proba[regex_pred]:.3f})")
        print(f"  Combined:         {combined_pred} (confidence: {combined_proba[combined_pred]:.3f})")
        print(f"  Status: Neural {neural_status} | Keyword {keyword_status} | Regex {regex_status} | Combined {combined_status}")

        results_list.append({
            'category': category,
            'text': text,
            'expected': expected,
            'neural_pred': neural_pred,
            'keyword_pred': keyword_pred,
            'regex_pred': regex_pred,
            'combined_pred': combined_pred,
            'neural_correct': neural_correct,
            'keyword_correct': keyword_correct,
            'regex_correct': regex_correct,
            'combined_correct': combined_correct,
        })

    # Print category summary
    print(f"\n{category} Summary:")
    print(f"  Neural Probe: {category_results['neural_correct']}/{category_results['total']}")
    print(f"  Keyword:      {category_results['keyword_correct']}/{category_results['total']}")
    print(f"  Regex:        {category_results['regex_correct']}/{category_results['total']}")
    print(f"  Combined:     {category_results['combined_correct']}/{category_results['total']}")

# ==============================================================================
# OVERALL SUMMARY
# ==============================================================================

print("\n" + "="*70)
print("OVERALL ADVERSARIAL RESULTS")
print("="*70)

results_df = pd.DataFrame(results_list)

print(f"\nTotal test cases: {len(results_df)}")
print(f"\nOverall Accuracy:")
print(f"  Neural Probe: {results_df['neural_correct'].sum()}/{len(results_df)} ({100*results_df['neural_correct'].mean():.1f}%)")
print(f"  Keyword:      {results_df['keyword_correct'].sum()}/{len(results_df)} ({100*results_df['keyword_correct'].mean():.1f}%)")
print(f"  Regex:        {results_df['regex_correct'].sum()}/{len(results_df)} ({100*results_df['regex_correct'].mean():.1f}%)")
print(f"  Combined:     {results_df['combined_correct'].sum()}/{len(results_df)} ({100*results_df['combined_correct'].mean():.1f}%)")

# Category-wise breakdown
print(f"\nBy Category:")
category_summary = results_df.groupby('category').agg({
    'neural_correct': 'mean',
    'keyword_correct': 'mean',
    'regex_correct': 'mean',
    'combined_correct': 'mean',
}).round(3)

category_summary.columns = ['Neural', 'Keyword', 'Regex', 'Combined']
print(category_summary)

# Cases where methods disagree
print(f"\n" + "="*70)
print("INTERESTING DISAGREEMENTS (Where Methods Differ)")
print("="*70)

# Find cases where neural probe differs from combined baseline
disagreements = results_df[results_df['neural_pred'] != results_df['combined_pred']]

if len(disagreements) > 0:
    print(f"\nFound {len(disagreements)} cases where Neural Probe differs from Combined Baseline:\n")
    for idx, row in disagreements.iterrows():
        status = "✓" if row['neural_correct'] else "✗"
        print(f"{status} \"{row['text']}\"")
        print(f"   Expected: {row['expected']}")
        print(f"   Neural: {row['neural_pred']} | Combined: {row['combined_pred']}")
        print()
else:
    print("\nNo disagreements found - methods are perfectly aligned!")

# Save results
results_df.to_csv('adversarial_test_results.csv', index=False)
print("\nSaved results to: adversarial_test_results.csv")